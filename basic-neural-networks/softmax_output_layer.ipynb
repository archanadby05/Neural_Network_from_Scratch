{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl7NVxhgiac5NgTvdRUDH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/archanadby05/Neural_Network_from_Scratch/blob/master/basic-neural-networks/softmax_output_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax Output Layer - Multi-Class Classification"
      ],
      "metadata": {
        "id": "0Bjw7Yj-2TP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **01. Define Softmax Activation for Output Layer**"
      ],
      "metadata": {
        "id": "HkkXjD2c2WrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax function is used for multi-class classification tasks. It converts logits (raw outputs of the network) into probabilities, making it useful for models where each output corresponds to a class."
      ],
      "metadata": {
        "id": "ixGJq8mi2YiM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CTMJrauO2Sth"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Softmax activation function\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))        # Subtracting max for numerical stability\n",
        "    return e_x / e_x.sum(axis=0, keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "\n",
        "Softmax squashes logits into a probability distribution, where the sum of the probabilities across all classes is 1. The subtraction of the maximum value before exponentiation ensures numerical stability during computation.\n",
        "\n"
      ],
      "metadata": {
        "id": "h7Tn8tmz2dgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **02. Implement Forward Pass with Logits to Probabilities**"
      ],
      "metadata": {
        "id": "b6YvDjna2fqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a class for the output layer that uses softmax to convert the raw network output (logits) into probabilities."
      ],
      "metadata": {
        "id": "L1cxucYf2h7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxOutputLayer:\n",
        "    def __init__(self, input_dim):\n",
        "        self.weights = np.zeros((input_dim, 3))  # For a 3-class problem\n",
        "        self.bias = np.zeros(3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = np.dot(x, self.weights) + self.bias\n",
        "        probabilities = softmax(logits)\n",
        "        return probabilities"
      ],
      "metadata": {
        "id": "DfETw80n2cL5"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "\n",
        "This class defines the output layer with weights and biases for a 3-class classification task. The forward pass computes the logits and then applies the softmax activation to obtain probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "g_20JvHo2lcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **03. Implement Categorical Cross-Entropy Loss**"
      ],
      "metadata": {
        "id": "iZtbB8SI2oy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The categorical cross-entropy loss function is commonly used for multi-class classification tasks. It measures the difference between the true label distribution and the predicted probabilities."
      ],
      "metadata": {
        "id": "P9XFsIyu2qs3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical cross-entropy loss\n",
        "def categorical_crossentropy(y_true, y_pred):\n",
        "\n",
        "    # Add small epsilon to avoid log(0)\n",
        "    return -np.sum(y_true * np.log(y_pred + 1e-15))"
      ],
      "metadata": {
        "id": "PHoUgOje2joX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "\n",
        "This loss function calculates the negative log likelihood between the true labels and the predicted probabilities. It’s crucial to prevent log(0) by adding a small epsilon value to the predicted probabilities."
      ],
      "metadata": {
        "id": "KIkGOqqZ2xjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **04. Use on Simple 3-Class Example and Validate Output**"
      ],
      "metadata": {
        "id": "-EpSdVn220S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s test our softmax output layer on a simple 3-class classification problem. We’ll define the input, expected output, and check the result."
      ],
      "metadata": {
        "id": "YLao7Zyi22uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple input for 3-class classification (e.g., 3 features)\n",
        "inputs = np.array([[2.0, 1.5, 0.7]])\n",
        "true_labels = np.array([[0, 0, 1]])  # Class 3 is the correct class\n",
        "\n",
        "# Initialize Softmax output layer\n",
        "output_layer = SoftmaxOutputLayer(input_dim=3)\n",
        "\n",
        "# Get probabilities from the model\n",
        "probabilities = output_layer.forward(inputs)\n",
        "\n",
        "# Calculate loss\n",
        "loss = categorical_crossentropy(true_labels, probabilities)\n",
        "\n",
        "print(\"Predicted probabilities:\", probabilities)\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZX9DotV2wO5",
        "outputId": "e87726ab-1734-4956-aff8-91174efbe9b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted probabilities: [[1. 1. 1.]]\n",
            "Loss: -1.110223024625156e-15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Explanation:*\n",
        "\n",
        "We create an example with 3 features and one correct class (class 3). After running the forward pass, we get the predicted probabilities. The loss is calculated using categorical cross-entropy, which tells us how well our predictions align with the true labels."
      ],
      "metadata": {
        "id": "PyE0N3e525pj"
      }
    }
  ]
}